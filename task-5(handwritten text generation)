import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation
from tensorflow.keras.optimizers import Adam

# Step 1: Load and preprocess the dataset
# Example: Using a custom dataset of handwritten-like transcriptions

# Load your dataset (for example, a corpus of handwritten text)
# If you have a file of text, you could read it using:
# text = open("handwritten_text.txt", "r").read()

# For simplicity, let's assume we're using a small dataset:
text = """hello world this is a simple text generation example using lstm"""

# Create a set of unique characters (vocabulary)
chars = sorted(set(text))
char_to_idx = {char: idx for idx, char in enumerate(chars)}
idx_to_char = {idx: char for idx, char in enumerate(chars)}

# Encode the text as integers
encoded_text = [char_to_idx[char] for char in text]

# Step 2: Prepare data for training (create sequences of input-output pairs)
sequence_length = 40  # Length of each input sequence
X = []
y = []

for i in range(len(encoded_text) - sequence_length):
    X.append(encoded_text[i:i + sequence_length])
    y.append(encoded_text[i + sequence_length])

X = np.array(X)
y = np.array(y)

# Reshape X to be suitable for LSTM (samples, timesteps, features)
X = np.expand_dims(X, axis=-1)  # (num_samples, sequence_length, 1)
y = tf.keras.utils.to_categorical(y, num_classes=len(chars))  # One-hot encode labels

# Step 3: Build the LSTM-based RNN model

model = Sequential()
model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))

# Step 4: Train the model
model.fit(X, y, batch_size=128, epochs=50, verbose=1)

# Step 5: Text Generation
def generate_text(model, start_string, num_generate=100):
    # Vectorize the start string
    input_eval = [char_to_idx[char] for char in start_string]
    input_eval = np.expand_dims(input_eval, axis=-1)

    # Empty string to hold the generated text
    generated_text = start_string

    # Generate characters one by one
    for _ in range(num_generate):
        predictions = model.predict(input_eval)
        predicted_id = np.argmax(predictions, axis=-1)[0]

        # Add predicted character to the generated text
        generated_text += idx_to_char[predicted_id]

        # Use the predicted character as the next input
        input_eval = np.expand_dims([predicted_id], axis=-1)

    return generated_text

# Step 6: Generate new text based on the learned patterns
start_string = "hello"
generated_text = generate_text(model, start_string, num_generate=200)

print("Generated text:\n", generated_text)
