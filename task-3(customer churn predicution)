import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# Step 1: Load the dataset (Replace with the actual dataset path)
data = pd.read_csv('customer_churn.csv')

# Step 2: Data Preprocessing
# Check for missing values
print("Missing values per column:")
print(data.isnull().sum())

# Convert 'Churn' to binary (if not already in 0/1 format)
data['Churn'] = data['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

# Split data into features (X) and target (y)
X = data.drop(columns=['CustomerID', 'Churn'])
y = data['Churn']

# Step 3: Preprocessing Pipeline
# Define preprocessing steps: scale numerical features, encode categorical features
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Create a column transformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),  # Standardize numerical columns
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # One-hot encode categorical columns
    ])

# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train models using a pipeline with preprocessing
# Logistic Regression
log_reg_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Random Forest
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Gradient Boosting
gb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', GradientBoostingClassifier(random_state=42))
])

# Step 6: Fit the models
log_reg_pipeline.fit(X_train, y_train)
rf_pipeline.fit(X_train, y_train)
gb_pipeline.fit(X_train, y_train)

# Step 7: Predict and evaluate the models

# Logistic Regression Predictions and Evaluation
y_pred_log_reg = log_reg_pipeline.predict(X_test)
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_log_reg))
print("Logistic Regression ROC-AUC:", roc_auc_score(y_test, y_pred_log_reg))

# Random Forest Predictions and Evaluation
y_pred_rf = rf_pipeline.predict(X_test)
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))
print("Random Forest ROC-AUC:", roc_auc_score(y_test, y_pred_rf))

# Gradient Boosting Predictions and Evaluation
y_pred_gb = gb_pipeline.predict(X_test)
print("Gradient Boosting Classification Report:\n", classification_report(y_test, y_pred_gb))
print("Gradient Boosting ROC-AUC:", roc_auc_score(y_test, y_pred_gb))
